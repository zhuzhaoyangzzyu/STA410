---
title: "A3"
author: "Zhu,Zhaoyang"
date: "November 11, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(splines)

```


##Q1

#a).

\(\hat tr (A) = \frac{1}{m}\sum_{i=1}^{m} V_i^TAV_i\)

\(Var(\hat tr (A)) = \frac{1}{m}Var(V^TAV)\)

In order to minimize \(\hat tr (A)\), we need to minimize \(Var(V^TAV)\).

\(Var(V^TAV) = E[(V^TAV)^2] - tr(A)^2\), since \(tr(A)^2\) is a constant \(\implies\) we just need to minimize \(E[(V^TAV)^2]\)

We know \(E[(V^TAV)^2] = \sum_{i=1}^n\sum_{j=1}^n\sum_{k=1}^n\sum_{l=1}^n a_{ij}a_{kl}E(V_iV_jV_kV_l)\)

We have two cases that \(E(V_iV_jV_kV_l) \neq 0\)

1. i=j=k=l: \(E(V_iV_jV_kV_l) = E(V_i^4) \implies E[(V^TAV)^2] = \sum_{i=1}^n a_{ii}^2 E(V_i^4)\)

2. i=j,k=l: \(E(V_iV_jV_kV_l) = E(V_i^2)E(V_j^2) = 1*1 \implies E[(V^TAV)^2] = \sum_{i=1}^n \sum_{k=1}^n a_{ii}^2 a_{kk}^2\), which is a constant.

\(\implies E[(V^TAV)^2] = \sum_{i=1}^n a_{ii}^2 E(V_i^4) + constant\)

Therefore, In order to minimize \(E[(V^TAV)^2]\) we need to minimize \(E(V_i^4)\)

\(Var(V_i^2) = E(V_i^4) - E^2(V_i^2) = E(V_i^4) - 1\)

\(\implies\) If we want to minimize \(E(V_i^4)\), we can just minimize \(Var(V_i^2)\)

\(Var(V_i^2)\) is minimized at 0.

Thus If we let our \(V_i\) = 1 or -1 with probability = \(\frac{1}{2}\) then:

1. \(E(V_i) = 1*\frac{1}{2} + (-1)*\frac{1}{2} = 0\)

2. \(E(V_i^2) = 1^2*\frac{1}{2} + (-1)^2*\frac{1}{2} = 1\)

3. \(E(V_i^4) = 1^4*\frac{1}{2} + (-1)^4*\frac{1}{2} = 1\)

4. \(Var(V_i^2) = E(V_i^4) - E^2(V_i^2) = 1 - 1 = 0\)

5. \(Var(V_i) = E(V_i^2) - E^2(V_i) = 1 - 0 = 1\)

All the assumptions and requirments are satisfied, therefore, If we choose our \(V_i = \pm 1, PR(V_i) = \frac{1}{2}\) Then we can minimize \(Var(\hat tr(A))\).

#b).
$$
H
\begin{pmatrix}
V \\
0
\end{pmatrix}
=
\begin{pmatrix}
H_{11} & H_{12}\\
H_{21} & H_{22}
\end{pmatrix}
\begin{pmatrix}
V \\
0
\end{pmatrix}
=
\begin{pmatrix}
H_{11}V + H_{12}*0 \\
H_{21}V + H_{22}*0
\end{pmatrix}
=
\begin{pmatrix}
H_{11}V  \\
H_{21}V
\end{pmatrix}
$$

$$
H
\begin{pmatrix}
H_{11}^{k-1}V \\
0
\end{pmatrix}
=
\begin{pmatrix}
H_{11} & H_{12}\\
H_{21} & H_{22}
\end{pmatrix}
\begin{pmatrix}
H_{11}^{k-1}V \\
0
\end{pmatrix}
=
\begin{pmatrix}
H_{11}^{k-1}V * H_{11} + H_{12}*0 \\
H_{11}^{k-1}V * H_{21} + H_{22}*0
\end{pmatrix}
=
\begin{pmatrix}
H_{11}^{k}V \\
 H_{21}H_{11}^{k-1}V
\end{pmatrix}
$$

#c).

```{r,echo=TRUE,eval=TRUE}
leverage <- function(x,w,r=10,m=100) {
               qrx <- qr(x)
               n <- nrow(x)
               lev <- NULL
               for (i in 1:m) {
                   set.seed(16) # keep the same v
                   v <- ifelse(runif(n)>0.5,1,-1)
                   v[-w] <- 0
                   v0 <- qr.fitted(qrx,v)
                   f <- v0
                   for (j in 2:r) {
                      v0[-w] <- 0
                      v0 <- qr.fitted(qrx,v0)
                      f <- f + v0/j
                      }
                   lev <- c(lev,sum(v*f))
                   }
                std.err <- exp(-mean(lev))*sd(lev)/sqrt(m)
                lev <- 1 - exp(-mean(lev))
                r <- list(lev=lev,std.err=std.err)
                r
}

x = c(1:1000)/1000
X1 = 1
for (i in 1:10) {
  X1 = cbind(X1,x^i)
}
X2 = cbind(1,bs(x,df=10))

plot(x,X2[,2])
for (i in 3:11){
  points(x,X2[,i])
}

leverage(X1,c(1:100))
leverage(X2,c(1:100)) 
```

From our output, we can see that our X1 has higher leverage than X2. The reason why to keep V the same, is because runif function will produce different V every time we run the function.

##Q2

#a).

MOM: \(\bar X = E(X), S^2 = Var(X) \implies \bar X = \frac{\alpha}{\lambda}, S^2 = \frac{\alpha}{\lambda^2} \implies \lambda = \frac{\bar X}{S^2},\alpha = \frac{\bar X^2}{S^2}\)

#b).
Define \(\Gamma^\prime \alpha = \phi(\alpha), \Gamma^{\prime\prime} \alpha = \phi^\prime(\alpha)\)

Likelihood Estimators:
\(l(\alpha,\lambda) = n\alpha log(\lambda) - nlog(\Gamma \alpha) + (\alpha - 1)\sum_{i=0}^{n-1}log(x_i) - \lambda\sum_{i=0}^{n-1}x_i\)

\(\frac{d}{d\alpha} = nlog(\lambda) + n\phi(\alpha) + \sum_{i=1}^{n}log(x_i) = 0 \implies \hat\alpha = \phi(\alpha) = \frac{1}{n}\sum_{i=1}^{n}log(x_i) + log(\lambda)\)

\(\frac{d}{d\lambda} = \frac{n\alpha}{\lambda} - \sum_{i=1}^{n}x_i = 0 \implies \hat\lambda = \frac{\alpha}{\bar x}\)

Newton-Raphson:

$$
score(\alpha,\lambda)= 
\begin{pmatrix}
\frac{dl}{d\alpha} \\
\frac{dl}{d\lambda}
\end{pmatrix}
=
\begin{pmatrix}
nlog(\lambda) + n\phi(\alpha) + \sum_{i=1}^{n}log(x_i)\\
\frac{n\alpha}{\lambda} - \sum_{i=1}^{n}x_i
\end{pmatrix}
$$

$$
-H= 
\begin{pmatrix}
-\frac{dl^2}{d\alpha^2} &&- \frac{dl^2}{d\alpha\lambda}\\
-\frac{dl^2}{d\lambda\alpha} && -\frac{dl^2}{d\lambda^2}
\end{pmatrix}
=
\begin{pmatrix}
n\phi^\prime(\alpha) && -\frac{n}{\lambda}\\
-\frac{n}{\lambda} && \frac{n\alpha}{\lambda^2}
\end{pmatrix}
$$

$$
\implies
\begin{pmatrix}
\hat\alpha_{k+1}\\
\hat\lambda_{k+1}\
\end{pmatrix}
=
\begin{pmatrix}
\hat\alpha_{k}\\
\hat\lambda_{k}\
\end{pmatrix}
+ (-H)^{-1}score(\hat\alpha_k,\hat\lambda_k)
$$


```{r,echo=TRUE,eval=TRUE}
mlegamma = function(x,eps=1.e-8,max.iter=500){
  n = length(x)
  alpha0 = mean(x)^2/var(x)
  lambda0 = mean(x)/var(x)
  scorealpha = sum(log(x)) + n*log(lambda0) - n*digamma(alpha0)
  scorelambda = -sum(x) + (n*alpha0)/(lambda0) 
  score = c(scorealpha,scorelambda)
  iter = 0
  alpha = alpha0
  lambda = lambda0
  theta = c(alpha0,lambda0)
  while ((max(abs(score))>eps && iter<=max.iter)){
    H_11 = n*trigamma(alpha) # minus H_11
    H_12 = -n/lambda # minus H_12
    H_21 = -n/lambda # minus H_21
    H_22 = (n*alpha)/(lambda)^2 #minus H_22
    H = matrix(c(H_11,H_12,H_21,H_22),ncol=2)
    theta = theta + solve(H,score)
    alpha = theta[1]
    lambda = theta[2]
    iter = iter + 1
    # update score
    scorealpha = sum(log(x)) + n*log(lambda) - n*digamma(alpha)
    scorelambda = -sum(x) + (n*alpha)/(lambda) 
    score = c(scorealpha,scorelambda)
  }
  H_11 =  n*trigamma(alpha)
  H_12 = -n/lambda
  H_21 = -n/lambda
  H_22 = (n*alpha)/(lambda)^2
  H = matrix(c(H_11,H_12,H_21,H_22),ncol=2)
  r = list(alpha=alpha,lambda=lambda,H=solve(H))
  r
}
mlegamma(rgamma(1000,shape = 1))
mlegamma(rgamma(1000,shape = 0.1))
mlegamma(rgamma(1000,shape = 5))
mlegamma(rgamma(1000,shape = 0.5))

```